{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GaussianGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnuevo/Itroduccion-GANs/blob/master/GaussianGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "s0SXQ2xRjw9T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introducción a los GANs -- En colaboración con VigoBrains\n",
        "\n",
        "__Gregorio Nuevo Castro__\n",
        "\n",
        "Este documento está basado en un workshop previo ([código aquí](https://github.com/Machine-Learning-Tokyo/Intro-to-GANs/blob/master/GaussianGAN.ipynb))."
      ]
    },
    {
      "metadata": {
        "id": "S4ygwdUDkMJ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "Bienvenido a esta _notebook_. En ella vamos a presentar brevemente el concepto de GANs y a ejecutar nuestro primer GAN. Pero antes de nada vamos a presentar las herramientas que vamos a utilizar:\n",
        "\n",
        "+ **Colab**, este entorno de ejecución parecido a las Jupyter notebooks ([https://colab.research.google.com/]()).\n",
        "+ **TensorFlow**, un framework para definir y ejecutar redes neuronales ([https://www.tensorflow.org/]()).\n",
        "\n",
        "## Importar dependencias\n",
        "\n",
        "Ahora importamos las dependencias que utilizaremos a lo largo del documento."
      ]
    },
    {
      "metadata": {
        "id": "yNlsZgjDlvUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# importar dependencias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, BatchNormalization, UpSampling2D, Conv2D, LeakyReLU, Flatten\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import widgets\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DRKC52bdlwyv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ejercicio propuesto\n",
        "\n",
        "Vamos a llevar a cabo un ejercicio muy simple. Queremos implementar un GAN para aproximar una [función Gaussiana](https://es.wikipedia.org/wiki/Funci%C3%B3n_gaussiana) en una dimensión. En la siguiente imagen podemos ver distintos ejemplos de funciones Gaussianas.\n",
        "![Funciones Gaussianas](https://camo.githubusercontent.com/4c94d1fc9852cdcbff7021b00f799d2f0aaf3e94/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f372f37342f4e6f726d616c5f446973747269627574696f6e5f5044462e7376672f3130383070782d4e6f726d616c5f446973747269627574696f6e5f5044462e7376672e706e67)\n",
        "\n",
        "Normalmente los GANs reciben un ruido de entrada z que sigue una distribución Uniforme o bien Gaussiana. En este ejercicio vamos a elegir una distribución Uniforme. Por qué? Bueno, si queremos aproximar una función Gaussiana a partir de otra Gaussiana, la transformación puede resultar trivial. En cambio, si introducimos un ruido z uniforme obligamos al GAN a aprender la transformación no uniforme de pasar de la forma de una distribución uniforme a la forma de una distribución Gaussiana."
      ]
    },
    {
      "metadata": {
        "id": "Yru8XW8snfKK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Empezando con los GANs\n",
        "\n",
        "Tras lo que hemos hablado en la charla, ya deberías estar un poco familiarizado con los GANs y con como funcionan. En resumidas cuentas, necesitamos dos redes neuronales que llamamos **Generador** y **Discriminador**. El **Generador** intenta imitar el tipo de muestras que hay en el dataset; mientras que el **Discriminador** intenta adivinar si una muestra en concreto es verdadera (legítima) o es falsa (creada por el **Generador**). \n",
        "\n",
        "Por ejemplo, en el caso de que en el dataset haya fotos de perros y gatos. El **Generador** intentaría aprender a generar fotos que parecen perros o gatos; mientras que el **Discriminador** aprendería a distinguir si una imagen concreta es real (pertenece al dataset) o falsa (es producto del **Generador**)\n",
        "\n",
        "Como sabemos, un GAN está formado por una serie de elementos:\n",
        "\n",
        "+ Nuestra **distribución objetivo**. Ésta es la distribución de probabilidad que queremos imitar, es la distribución de probabilidad que aparece en el dataset. En el ejemplo de los perros y gatos sería la distribución de probabilidad de los píxeles para hacer que una imagen parezca un perro o un gato. Recuerda p(y|x) y p(x,y). En nuestro ejemplo concreto la distribución de probabilidad que queremos imitar es una función Gaussiana.\n",
        "+ Un **muestreador de ruido**. Ésta es la entrada a nuestro **Generador**. Necesitamos un generador de números aleatorios que siga una distribución dada. En nuestro caso hemos dicho que vamos a utilizar una distribución Uniforme. Podemos hacer que esta distribución tenga tantas dimensiones como queramos. En general, será un vector de N dimensiones de distribución uniforme. Esto es ALGO IMPORTANTE QUE DEBES TENER EN MENTE.\n",
        "+ El **Discriminador**, como ya sabemos es una red neuronal que distingue entre muestras reales (vienen del dataset) o falsas (son producto del generador).\n",
        "+ El **Generador**, es una red neuronal que aprende a producir muestras que parecen reales a partir de ruido no correlacionado.\n",
        "\n",
        "Al principio puede resultar extraño entender qué son el **Generador** y el **Discriminador**. Es decir, hacen cosas que molan mucho. Entonces, tienen algo de especial? No, para nada! El **Discriminador** es un simple classificador binario: 0 si creo que la muestra es falsa; 1 si creo que es verdadera. Y el **Generador** es una red neuronal que recibe ruido de entrada y devuelve un tensor con la misma forma que las muestras en el dataset. En ese caso, ¿dónde está la mágia de los GANs? El truco está en como interactúan el **Generador** y el **Discriminador** entre sí para llevar a cabo su tarea.\n",
        "\n",
        "### Distribución objetivo y ruido de entrada\n",
        "\n",
        "Ahora que ya sabemos bastante sobre nuestro GAN llega el momento de empezar a escribir algo de código. Lo primero que vamos a hacer es definir funciones para generar la distribución objetivo y la distribución de ruido de entrada.\n",
        "\n",
        "+ **Distribución objetivo**, función Gaussiana de la que podemos elegir la _media_ y la desviación estándar _std_.\n",
        "+ **Distribución de ruido**, muestras uniformes entre 0 y 1."
      ]
    },
    {
      "metadata": {
        "id": "lgFg_QGHndpq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Uniform sampler ###\n",
        "# Our noise sampler (Z) is a function that returns a vector of a desired length \n",
        "# with uniform samples between 0 and 1\n",
        "# we can use m and n to ask for a matrix of Uniform samples with the desired shape\n",
        "# therefore m is the dimension of the noise\n",
        "def uniform_sampler():\n",
        "  return lambda m, n: np.random.uniform(low=0.0, high=1.0, size=(m, n))\n",
        "\n",
        "# we set the sampler to a variable so we can change it later if we want\n",
        "noise_sampler = uniform_sampler()\n",
        "# this is a 10x5 matrix with Uniform samples\n",
        "uniform_matrix = noise_sampler(10, 5)\n",
        "\n",
        "\n",
        "### Gaussian sampler ###\n",
        "# Our Gaussian sampler is a function that returns a vector of Gaussian samples\n",
        "# with a specific mean and std\n",
        "# We can use n to ask for a vector of shape 1xn\n",
        "def gaussian_sampler(mu, sigma):\n",
        "  return lambda n: np.random.normal(mu, sigma, (1, n))\n",
        "\n",
        "# again, set it to a variable so we can change it later\n",
        "mu, sigma = 2, 1\n",
        "data_sampler = gaussian_sampler(mu, sigma)\n",
        "# this is a 1x5 vector with Gaussian samples\n",
        "gaussian_vector = data_sampler(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kLEDIAMu9oh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ahora podemos echarle un vistazo a los datos generados utilizando histogramas. Nuestras funciones devuelven matrices de números, pero a través de los histogramas deberíamos ser capaces de observar las formas características de las distribuciones Gaussiana y Uniforme.\n",
        "\n",
        "A través del siguiente formulario de colab puedes configurar algunos valores de la distribución para ajustarla a gusto.\n",
        "\n",
        "+ **num_samples**, número de muestras a dibujar. Cuántas más muestras se dibujen más se pareceran los histogramas a la distribución de probabilidad correcta.\n",
        "+ **gaussian_mean** y **gaussian_std**, parámetros de la distribución Gaussiana\n",
        "+ **num_bins**, número de bloques del histograma\n",
        "+ **colores**, echa un vistazo y elije tus favoritos ;)\n",
        "\n",
        "Experimenta cambiando los valores de los parámetros y viendo cómo afectan al resultado. Es importante que te familiarices con estos conceptos de cara a los siguientes pasos."
      ]
    },
    {
      "metadata": {
        "id": "ZD98hO4pvk64",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define parameters\n",
        "num_samples = 6761 #@param {type:\"slider\", min:1, max:10000, step:10}\n",
        "gaussian_mean = 3 #@param {type:\"slider\", min:-3, max:3, step:0.2}\n",
        "gaussian_std = 2.7 #@param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "\n",
        "num_bins = 100 #@param {type:\"slider\", min: 1, max:100, step:1}\n",
        "my_favourite_color_is = 'red' #@param [\"green\", \"blue\", \"red\", \"yellow\", \"black\", \"cyan\", \"magenta\"] {allow-input: false}\n",
        "but_I_also_like = 'blue' #@param [\"green\", \"blue\", \"red\", \"yellow\", \"black\", \"cyan\", \"magenta\"] {allow-input: false}\n",
        "\n",
        "# this creates a grid to make 2 plots side by side\n",
        "grid = widgets.Grid(1,2)\n",
        "\n",
        "# generate samples with the chosen characteristics\n",
        "noise = noise_sampler(1, num_samples)\n",
        "noise = np.reshape(noise, num_samples)\n",
        "data_sampler = gaussian_sampler(gaussian_mean, gaussian_std)\n",
        "data = data_sampler(num_samples)\n",
        "data = np.reshape(data, num_samples)\n",
        "\n",
        "# plot our data\n",
        "with grid.output_to(0, 0):\n",
        "  n, bins, patches = plt.hist(noise, num_bins, density=True, facecolor=my_favourite_color_is,\n",
        "                                alpha=0.75)\n",
        "  ax = plt.gca()\n",
        "  ax.set_xlim(-0.25,1.25)\n",
        "  ax.set_ylim(0, 1.5)\n",
        "  plt.title(\"Noise distribution (Uniform between 0 and 1)\")\n",
        "  \n",
        "with grid.output_to(0, 1):\n",
        "  n, bins, patches = plt.hist(data, num_bins, density=True, facecolor=but_I_also_like,\n",
        "                                alpha=0.75)\n",
        "  ax = plt.gca()\n",
        "  ax.set_xlim(-10,10)\n",
        "  ax.set_ylim(0, 0.7)\n",
        "  plt.title(\"Data distribution (Gaussian with mean={} and std={})\".format(gaussian_mean, gaussian_std))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOczdmZoyHlD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Espero que ahora tengas mucho más claro los tipos de distribuciones con los que estamos trabajando.\n",
        "\n",
        "## Definir las redes neuronales\n",
        "\n",
        "Vamos a programar el **Discriminador** y el **Generador**.\n",
        "\n",
        "### Discriminador\n",
        "\n",
        "El **Discriminador** tiene que distinguir si una muestra es real (1) o falsa (0). ¿Suena familiar? Eso es un clasificador binario. De hecho, nuestro **Discriminador** no es más que un clasificador binario.\n",
        "\n",
        "Aquí tenemos que hacer una pequeña pausa para ver el mundo desde el punto de vista del **Discriminador**. ¿Qué sucederá si el **Discriminador** recibe muestras (tanto reales como falsas) una a una? Pues que difícilmente será capaz de hacerse una idea de la distribución que siguen. Podemos verlo nosotros mismos con la herramienta de visualización de arriba. Reduce el valor de `num_bins` paulatinamente hasta llegar a `1`. Cuanto más pequeño es el valor de `num_bins` más difícil es distinguir la forma verdadera de la distribución. Ahora reduce también `num_samples` a `1`. Con esta configuración es completamente imposible distinguir ambas distribuciones. Puede que un humano sea capaz de hacerlo, pero eso es probablemente porque nosotros contamos con memoria. El **Discriminador** carece de ningún tipo de memoria, por lo que bajo estas condiciones le resultará muy difícil distinguir entre muestras reales y muestras falsas.\n",
        "\n",
        "Si el **Discriminador** recibe las muestras una a una, probablemente no haga un buen trabajo. Para ello podemos darle las muestras en grupos. Si el **Discriminador** ve un grupo de muestras de una Gaussiana será capaz de observar la forma característica de la distribución. Al igual que a nosotros nos resulta mucho más informativo un histograma de muchas muestras que no observar muestras una a una. A esta técnica se la llama _minibatch discrimination_ y la vamos a utilizar para mejorar el funcionamiento de nuestro **Discriminador**.\n",
        "\n",
        "Ahora podemos programar nuestro **Discriminador** con las siguientes características:\n",
        "\n",
        "+ Acepta una entrada con forma `minibatch_size` (el grupo de muestras de nuestro _minibatch discrimination_)\n",
        "+ Tiene 3 capas _fully connected_\n",
        "+ Las activaciones son `elu` para las capas intermedias y `sigmoid` para la capa de salida\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iib_qNT75bYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_discriminator(input_size=100, hidden_size=50, output_size=1):\n",
        "  return Sequential([\n",
        "    Dense(hidden_size, activation='elu', input_shape=(input_size,)),\n",
        "    Dense(hidden_size, activation='elu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uVpXxfVtE_xK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generador\n",
        "\n",
        "En cuanto al **Generador** tenemos lo siguiente. Se trata de una red que recibe un vector de ruido Uniforme (de N dimensiones) y devuelve lo que sería una muestra falsa. Vamos a definir nuestro **Generador** con una arquitectura similar al **Discriminador**. En concreto, nuestro **Generador**\n",
        "\n",
        "+ Recibe \n",
        "+ Tiene 3 capas _fully connected_.\n",
        "+ La activación es `elu` para las capas internas, y `linear` para la última capa. ¿Por qué la última capa no tiene activación? Bueno, las funciones de activación normalmente limitan el rango de valores de la salida (por ejemplo `relu` y `elu` a valores positivos, `sigmoid` a valores entre 0 y 1, etc.). Eso no es lo que nos interesa. En este caso, si queremos imitar una función uniforme sabemos que el rango de valores potencial es desde `-infinito` a `+infinito++."
      ]
    },
    {
      "metadata": {
        "id": "1VHk4hC23lmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_generator(input_size=1, hidden_size=50, output_size=100):\n",
        "  return Sequential([\n",
        "    Dense(hidden_size, activation='elu', input_shape=(input_size,)),\n",
        "    Dense(hidden_size, activation='elu'),\n",
        "    Dense(output_size, activation='linear')\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L-bdcI4qYv_A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compilar los modelos\n",
        "\n",
        "En `TensorFlow` y otros _frameworks_ para ejecutar redes neuronales, tenemos que compilar los modelos antes de utilizarlos. Es decir, para utilizar nuestro **Generador** y nuestro **Discriminador** no sólo vale con definir su arquitectura, si no que también necesitamos compilarlo. Al compilarlo, se crea el grafo de ejecución de la red neuronal y se realizan optimizaciones para acelerar su ejecución, entre otras. (Nota, no todos los _frameworks_ necesitan de este paso de compilado; algunos, como `PyTorch` carecen de él; el mismo `TensorFlow` está experimentando con un modo de ejecución llamado `Eager Execution` que también carece de este paso de compilación).\n",
        "\n",
        "Compilar las partes de un GAN resulta un proceso un tanto verboso. No sólo hay que compilar **Discriminador** y **Generador** por su lado, si no también un modelo conjunto o combinado (`combined`). Este modelo combinado será el encargado de entrenar el **Generador**. ¿Por qué? Pues porque para entrenar el **Generador** necesitamos _combinar_ tanto el **Generador** en sí (para producir una muestra) como el **Discriminador** (para que haga una valoración de la muestra producida)."
      ]
    },
    {
      "metadata": {
        "id": "EKK7-7u4a6VH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compile_models(generator, discriminator, noise_length):\n",
        "  assert generator.layers[-1].output.shape[1] == discriminator.layers[0].input.shape[1], (\"The output shape of generator must match the input shape of discriminator\")\n",
        "  \n",
        "  adam = Adam(lr=0.0001, beta_1=0.5)\n",
        "\n",
        "  generator.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "\n",
        "  discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "  z = Input(shape=(noise_length,))\n",
        "  img = generator(z)\n",
        "  discriminator.trainable = False\n",
        "  score = discriminator(img)\n",
        "\n",
        "  combined = Model(inputs=[z], outputs=[score])\n",
        "  combined.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "\n",
        "  return generator, discriminator, combined  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9-KKC0TYaS2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xygvXChM-9Ed",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Notas sobre minibatch discrimination y la implementación en Keras\n",
        "\n",
        "Hemos hablado arriba sobre `minibatch discrimination`. Este proceso nos permite hacer la discriminación de muestras sobre todo un minibatch en lugar de hacerlo muestra a muestra. Así, debería resultar mucho más fácil distinguir las distribuciones de probabilidad real y falsa, y por tanto esto ayuda al entrenamiento.\n",
        "\n",
        "Esto significa que nuestro **Discriminador**, en lugar de recibir una única muestra y devolver un score (1 -> 1) recibiría, por ejemplo, un minibatch de 100 muestras y devolvería un score (100 -> 1). Esto, a priori, no cambia el comportamiento de nuestro **Generador**. El **Generador** recibe una muestra de ruido Uniforme (de N dimensiones) y genera una **única** muestra de la distribución falsa a partir de ella (N -> 1).\n",
        "\n",
        "Esto funciona muy bien mientras estamos entrenando el **Discriminador**. Entonces podemos generar `batch_size` muestras utilizando el **Generador** y dárselas al **Discriminador** para que las valore. Lamentablemente, esto no funciona cuando para entrenar el **Generador** dado que tenemos que utilizar el modelo **Combinado**. Imagínatelo, para entrenar el **Generador** a través del modelo combinado en el caso anterior produciríamos 100 muestras Uniformes (de N dimensiones), con ellas el **Generador** produciría 100 muestras falsas y el **Discriminador** cogería esas 100 muestras de golpe para darnos una valoración.  Entonces nuestro modelo **Combinado** sería tal que\n",
        "\n",
        "```\n",
        "              +------------+                 +---------------+\n",
        "              |            |                 |               |\n",
        "  Uniforme +--> Generador  +--> Fake dist +--> Discriminador +-->  Score\n",
        "  (100, N)    |            |    (100,)       |               |     (1,)\n",
        "              +------------+                 +---------------+\n",
        "\n",
        "```\n",
        "\n",
        "La entrada equivale a un minibatch de 100 muestras (100, N) mientras que la salida equivale a un minibatch de 1 muestra (1,). Esto no está permitido en Keras (aunque sí lo está en PyTorch, puedes echar un vistazo al [código en el que se basa este ejercicio](https://github.com/Machine-Learning-Tokyo/Intro-to-GANs/blob/master/GaussianGAN.ipynb)). Por tanto se ha decidido cambiar el código para que el **Generador**, en lugar de generar las muestras una a una (1 -> 1) lo haga minibatch a minibatch a partir de una única muestra uniforme (1 -> 100). \n",
        "\n",
        "```\n",
        "              +------------+                 +---------------+\n",
        "              |            |                 |               |\n",
        "  Uniforme +--> Generador  +--> Fake dist +--> Discriminador +-->  Score\n",
        "  (1, N)      |            |    (100,)       |               |     (1,)\n",
        "              +------------+                 +---------------+\n",
        "\n",
        "```\n",
        "\n",
        "Este cambio nos permite escribir el código en Keras. Sin embargo, al producir minibatch muestras a la vez con el **Generador**, todas estas muestras estarán correlacionadas. Esto es algo que en principio no es deseable. Por otra parte. Al producir las muestras minibatch a minibatch, al **Generador** le resulta mucho más fácil entender las características de la distribución que debe imitar y por tanto facilita el entrenamiento."
      ]
    },
    {
      "metadata": {
        "id": "NzC89N_OzGA2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utility functions\n",
        "\n",
        "Estas funciones nos permiten realizar _plotting_ de los resultados. No hace falta que las entiendas pero por si estás interesado\n",
        "\n",
        "+ `make_histogram()` coge dos arrays con datos sobre una y otra distribución (real y falsa) y dibuja el histograma de ambas. En este caso, el histograma nos ayuda a observar la distribución de probabilidad de los datos.\n",
        "\n",
        "+ `plot_errors()` dibuja las gráficas de error de los errores del discriminador y del generador.\n"
      ]
    },
    {
      "metadata": {
        "id": "0R8ouMJXzT9z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# utility function to plot the histograms of training\n",
        "def make_histogram(data_real, data_fake, i, n_bins=100):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    axes = plt.subplot(111)\n",
        "    n, bins, patches = plt.hist(data_real, n_bins, normed=1, facecolor='green',\n",
        "                                alpha=0.75, label=\"Real data\")\n",
        "    n, bins, patches = plt.hist(data_fake, bins, normed=1, facecolor='blue',\n",
        "                                alpha=0.75, label=\"Generated data\")\n",
        "    axes.set_xlim(-5, 8)\n",
        "    axes.set_ylim(0, 1)\n",
        "    plt.title(\"Probability distribution of real and fake data\")\n",
        "    plt.legend([\"Real data\", \"Generated data\"])\n",
        "    plt.tight_layout()\n",
        "\n",
        "# utility function to plot the evolution of the errors during training\n",
        "def plot_errors(real_loss_buffer, fake_loss_buffer, g_loss_buffer):\n",
        "    fig = plt.figure(0, figsize=(10, 10))\n",
        "    plt.subplot(311)\n",
        "    plt.plot(real_loss_buffer)\n",
        "    plt.title(\"D real loss\")\n",
        "\n",
        "    plt.subplot(312)\n",
        "    plt.plot(fake_loss_buffer)\n",
        "    plt.title(\"D fake loss\")\n",
        "\n",
        "    plt.subplot(313)\n",
        "    plt.plot(g_loss_buffer)\n",
        "    plt.title(\"G fake loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SoAJ3P-UzWmy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hiperparámetros\n",
        "\n",
        "El siguiente formulario nos permite cambiar algunos de los hiperparámetros de los que depende el sistema.\n",
        "\n",
        "La configuración por defecto funciona correctamente, pero puedes cambiarlos para ver como influencian el entrenamiento."
      ]
    },
    {
      "metadata": {
        "id": "XUsdWbujIli4",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "param = namedtuple(\"Parameters\",\n",
        "                  \"iterations mu sigma hidden_size batch_size input_size\")\n",
        "\n",
        "#@title Define hyperparameters\n",
        "param.iterations = 19000 #@param {type:\"slider\", min:1000, max:50000, step:2000}\n",
        "param.print_step = 400 #@param {type:\"slider\", min:50, max:1000, step:50}\n",
        "param.mu = 1.5 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "param.sigma = 1 #@param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "param.hidden_size = 50 #@param {type:\"slider\", min:10, max:100, step:5}\n",
        "param.batch_size = 100 #@param {type:\"slider\", min:10, max:1000, step:10}\n",
        "param.noise_size = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "\n",
        "# print some results every 200 steps\n",
        "print_step = 200\n",
        "\n",
        "# these are some hyperparameters to for the network architecture\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "\n",
        "learning_rate = 2e-4\n",
        "batch_size = 100\n",
        "test_num = 10000 # number of samples used for testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hwAY80qBCuPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Función de entrenamiento"
      ]
    },
    {
      "metadata": {
        "id": "F41rK-3QDsKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  # this variable is for display purposes, you can ignore it!\n",
        "  grid = widgets.Grid(2,2)\n",
        "  global train_gaussian, train_generation, train_errors\n",
        "  train_generation = []\n",
        "  train_errors = []\n",
        "\n",
        "  # create and compile networks\n",
        "  discriminator = create_discriminator(param.batch_size, param.hidden_size, 1)\n",
        "  generator = create_generator(param.noise_size, param.hidden_size, param.batch_size)\n",
        "  G, D, C = compile_models(generator, discriminator, param.noise_size)\n",
        "  print('G', G.input_shape, '->', G.output_shape)\n",
        "  print('D', D.input_shape, '->', D.output_shape)\n",
        "  \n",
        "  # get our data and noise samplers\n",
        "  data_sampler = gaussian_sampler(param.mu, param.sigma)\n",
        "  test_data = data_sampler(test_num)\n",
        "  train_gaussian = test_data\n",
        "    \n",
        "  # this list will keep track of the errors during training\n",
        "  real_loss_buffer = []\n",
        "  fake_loss_buffer = []\n",
        "  g_loss_buffer = []\n",
        "  \n",
        "  global count\n",
        "  count = 0\n",
        "  \n",
        "  for i in range(param.iterations):\n",
        "    ## Train Discriminator on real samples\n",
        "    # take real samples\n",
        "    real_data = data_sampler(param.batch_size)\n",
        "    # train discriminator on real images\n",
        "    d_real_targets = np.ones(1)\n",
        "    real_loss = D.train_on_batch(real_data, d_real_targets)\n",
        "    real_loss_buffer.append(real_loss[0])\n",
        "    \n",
        "    ## Train Discriminator on fake samples\n",
        "    z = noise_sampler(1, param.noise_size)\n",
        "    fake_data = G.predict(z)\n",
        "    d_fake_targets = np.zeros(len(fake_data))\n",
        "    fake_loss = D.train_on_batch(fake_data, d_fake_targets)\n",
        "    fake_loss_buffer.append(fake_loss[0])\n",
        "    \n",
        "    ## Train Generator\n",
        "    z = noise_sampler(1, param.noise_size)\n",
        "    g_targets = np.ones(1)\n",
        "    g_loss = C.train_on_batch(z, g_targets)\n",
        "    g_loss_buffer.append(g_loss)\n",
        "    \n",
        "    \n",
        "    ## Visualise training--------------\n",
        "    if (i + 1) % param.print_step == 0:\n",
        "\n",
        "      # print current errors\n",
        "      with grid.output_to(1,0):\n",
        "        if (i + 1) % (15 * param.print_step) == 0:\n",
        "          grid.clear_cell()\n",
        "        print(\"{:>6} -> D_real_loss {:.6f}  D_fake_loss {:.6f}  \"\n",
        "              \"G_fake_loss {:.6f}\".format(\n",
        "          i+1,\n",
        "          real_loss_buffer[-1],\n",
        "          fake_loss_buffer[-1],\n",
        "          g_loss_buffer[-1]\n",
        "        ))\n",
        "          \n",
        "      # plot distributions\n",
        "      fake_data = np.array([[]]) # empty array\n",
        "      for _ in range(test_num // param.batch_size):\n",
        "        z_test = noise_sampler(1, param.noise_size)\n",
        "        new_fake_data = G.predict(z_test)\n",
        "        fake_data = np.concatenate((fake_data, new_fake_data), axis=1)\n",
        "      \n",
        "      with grid.output_to(0, 0):\n",
        "        grid.clear_cell()\n",
        "        \n",
        "        make_histogram(\n",
        "            test_data[0,:], \n",
        "            fake_data[0,:], \n",
        "            i)\n",
        "        plt.savefig(\"figure_{}.png\".format(count))\n",
        "        count += 1\n",
        "\n",
        "      # plot training errors\n",
        "      with grid.output_to(0, 1):\n",
        "        grid.clear_cell()\n",
        "        plot_errors(real_loss_buffer, fake_loss_buffer, g_loss_buffer) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuc3Lh1QIolk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ejecutar el código\n",
        "\n",
        "Ahora que ya está todo preparado, podemos ejecutar el código."
      ]
    },
    {
      "metadata": {
        "id": "F2LkD2xNIny3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPZvDJ8xIwZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Análisis de los resultados\n",
        "\n",
        "Según el GAN empiece a entrenar verás 3 cuadrados con información:\n",
        "\n",
        "+ Arriba a la izquierda se muestra el histograma de ambas distribuciones (o lo que es lo mismo, la distribución de probabilidad de los datos reales y falsos) en el instante concreto de entreanmiento. Si todo va bien, la gráfica azul debería parecerse cada vez más a la verde.\n",
        "+ Arriba a la derecha se muestran las gráficas de errores y su evolución en el tiempo: el error del discriminador al clasificar muestras reales, el error del discriminador al clasificar muestras falsas, y el error del generador.\n",
        "+ Abajo a la izquierda se muestra el valor numérico de los errores según la iteración actual.\n",
        "\n",
        "#### Cosas en las que fijarse\n",
        "\n",
        "+ ¿Cómo aprende el GAN?\n",
        "+ ¿En cada nueva iteración, el GAN lo hace siempre mejor que en las versiones anteriores o hay veces que no?\n",
        "+ ¿Qué pasa con los _plots_ de los errores? ¿Cómo es que a veces suben, otras veces bajan, a veces parece que oscilan como locos?\n",
        "+ Si los errores son tan extraños ¿cómo sé cuándo puedo parar de entrenar el GAN? ¿cuándo termino el entrenamiento?\n",
        "+ ¿Si entrenamos el GAN hasta la perfección (las distribuciones real y falsa completamente indistinguibles) cuál será el error de l **Discriminador**?"
      ]
    },
    {
      "metadata": {
        "id": "eeNInCDhI9PA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Volver a probar\n",
        "\n",
        "Para que no tengas que ejecutar varias celdas para entrenar, aquí se ha puesto todo en una única celda. Simplemente configura los hiperparámetros que quieras y ejecuta la celda para entrenar el GAN."
      ]
    },
    {
      "metadata": {
        "id": "PK1I1kpMKGN1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "param = namedtuple(\"Parameters\",\n",
        "                  \"iterations mu sigma hidden_size batch_size input_size\")\n",
        "\n",
        "#@title Define hyperparameters\n",
        "param.iterations = 19000 #@param {type:\"slider\", min:1000, max:50000, step:2000}\n",
        "param.print_step = 400 #@param {type:\"slider\", min:50, max:1000, step:50}\n",
        "param.mu = 3.2 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "param.sigma = 0.4 #@param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "param.hidden_size = 50 #@param {type:\"slider\", min:10, max:100, step:5}\n",
        "param.batch_size = 100 #@param {type:\"slider\", min:10, max:1000, step:10}\n",
        "param.noise_size = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "\n",
        "# print some results every 200 steps\n",
        "print_step = 200\n",
        "\n",
        "# these are some hyperparameters to for the network architecture\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "\n",
        "learning_rate = 2e-4\n",
        "batch_size = 100\n",
        "test_num = 10000 # number of samples used for testing\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vr02KEsLI_GQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hacer un vídeo\n",
        "\n",
        "Ahora vamos a visualizar el entrenamiento haciendo una animación. Mientras se estaba entrenando el GAN, hemos hecho capturas de las imágenes. Veamos cuantas tenemos guardadas."
      ]
    },
    {
      "metadata": {
        "id": "pNXb8XtOrpwB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!ls | wc -l\n",
        "print(\"Number of images saved\", count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZwW3nk-GijwF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reseteo de emergencia\n",
        "\n",
        "El siguiente formulario es un reseteo de emergencia. Es posible que haya habido algún fallo y hayas tenido que reiniciar el entorno de ejecución. Si ha sido así, se ha perdido información de contexto relevante para generar la animación. Marcando el `restart_checkbox` y ejecutando el siguiente código se debería recuperar esa información de contexto automáticamente."
      ]
    },
    {
      "metadata": {
        "id": "pCTD5ZF3LnwK",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Did you have to restart? (check and run if you did)\n",
        "restart_checkbox = False #@param {type:\"boolean\"}\n",
        "\n",
        "if restart_checkbox:\n",
        "  # import again\n",
        "  print(\"Importing modules\")\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from google.colab import widgets\n",
        "  import matplotlib.gridspec as gridspec\n",
        "  from matplotlib import animation, rc\n",
        "  from IPython.display import HTML\n",
        "  from PIL import Image\n",
        "  \n",
        "  # read files\n",
        "  print(\"Reading the number of images\")\n",
        "  from os import listdir\n",
        "  count = len(list(filter(lambda name: \"figure_\" in name, listdir())))\n",
        "  print(\"Number of images saved\", count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWZ9A9NnjHBM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### El objeto de animación\n",
        "\n",
        "La siguiente celda contiene el objeto encargado de realizar la animación. Tan solo tienes que ejecutarla."
      ]
    },
    {
      "metadata": {
        "id": "Y0rle5lqLtnf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AnimObject(object):\n",
        "    def __init__(self, images):\n",
        "        print(len(images))\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "        self.ax.set_title(\"\")\n",
        "        self.fig.set_size_inches((8, 8))\n",
        "        self.plot = plt.imshow(images[0])\n",
        "        plt.tight_layout()\n",
        "        self.images = images\n",
        "        \n",
        "    def init(self):\n",
        "        self.plot.set_data(self.images[0])\n",
        "        self.ax.grid(False)\n",
        "        return (self.plot,)\n",
        "      \n",
        "    def animate(self, i):\n",
        "        self.plot.set_data(self.images[i])\n",
        "        self.ax.grid(False)\n",
        "        self.ax.set_xticks([])\n",
        "        self.ax.set_yticks([])\n",
        "        self.ax.set_title(\"index {}\".format(i))\n",
        "        return (self.plot,)\n",
        "\n",
        "def get_figures(template, indices):\n",
        "    import os.path\n",
        "    images = []\n",
        "    for index in indices:\n",
        "        if os.path.isfile(template.format(index)):\n",
        "            images.append(Image.open(template.format(index)))\n",
        "    return images\n",
        "  \n",
        "images = get_figures(\"figure_{}.png\", range(count))\n",
        "animobject = AnimObject(images)\n",
        "anim = animation.FuncAnimation(\n",
        "              animobject.fig,\n",
        "              animobject.animate,\n",
        "              frames=len(animobject.images),\n",
        "              interval=150,\n",
        "              blit=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvEwWxf6LwSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizar la animación\n",
        "\n",
        "Ejecuta la siguiente celda para ver la animación."
      ]
    },
    {
      "metadata": {
        "id": "cys3naY_LvrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}